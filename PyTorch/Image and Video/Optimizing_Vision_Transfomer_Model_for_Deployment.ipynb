{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimizing Vision Transfomer Model for Deployment",
      "provenance": [],
      "authorship_tag": "ABX9TyN0bFpo79FajADYNVWn/Krb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0d880d1fc244d9585a408b88082fcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9685d458988645eaaeacb64ae9b44d58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ea15eba4e85748109483867c89022f20",
              "IPY_MODEL_13b3d987fdd940c884a814f33e919986",
              "IPY_MODEL_ef94a399729f469785a84ddebbe1e931"
            ]
          }
        },
        "9685d458988645eaaeacb64ae9b44d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea15eba4e85748109483867c89022f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea4ba6ab392945d2929004334fddd59b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38305f2f5f0d4c79b69c10bab0f43661"
          }
        },
        "13b3d987fdd940c884a814f33e919986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b54ff3fd6194cf284fb04931fe8ee61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 346319111,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 346319111,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69af8362188e4ec98b896182c9b6299b"
          }
        },
        "ef94a399729f469785a84ddebbe1e931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_80db6ea78e2648df82832c15291bc6a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 330M/330M [00:06&lt;00:00, 50.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a2d0b98a117421fa7cc1dcbc9dcf5ed"
          }
        },
        "ea4ba6ab392945d2929004334fddd59b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38305f2f5f0d4c79b69c10bab0f43661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b54ff3fd6194cf284fb04931fe8ee61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69af8362188e4ec98b896182c9b6299b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80db6ea78e2648df82832c15291bc6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a2d0b98a117421fa7cc1dcbc9dcf5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChoiDae1/2022_CVLAB_Winter_Study/blob/main/PyTorch/Image%20and%20Video/Optimizing_Vision_Transfomer_Model_for_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to DeiT (Data efficient image Transfomer)\n",
        "\n",
        "**Vision Transfomer** -> 훈련에 있어서 엄청나게 많은 양의 데이터가 필요함 -> 활용에 한계가 있음\n",
        "\n",
        "</br> **DeiT** -> Vision Transfomer의 구조를 그대로 사용하지만, **knowledge distillation**을 통해, 적은 양의 데이터로도 SOTA에 준하는 성능을 낼 수 있음\n",
        "(knowledge distillation: Teacher model과 Student model을 각각 두어, Student model이 Teacher model의 학습지식을 배울 수 있도록 하는 학습기법)"
      ],
      "metadata": {
        "id": "Uhi3TN8CSfni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm pandas requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXn43JIaTmX8",
        "outputId": "cd20daa3-c56d-43c6-88c7-eb2cd18a89c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifying Images with DeiT"
      ],
      "metadata": {
        "id": "SW0DTN3qVKdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import timm\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "print(torch.__version__)\n",
        "# should be 1.8.0\n",
        "\n",
        "\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=3),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "])\n",
        "\n",
        "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
        "img = transform(img)[None,]\n",
        "out = model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "c0d880d1fc244d9585a408b88082fcbe",
            "9685d458988645eaaeacb64ae9b44d58",
            "ea15eba4e85748109483867c89022f20",
            "13b3d987fdd940c884a814f33e919986",
            "ef94a399729f469785a84ddebbe1e931",
            "ea4ba6ab392945d2929004334fddd59b",
            "38305f2f5f0d4c79b69c10bab0f43661",
            "5b54ff3fd6194cf284fb04931fe8ee61",
            "69af8362188e4ec98b896182c9b6299b",
            "80db6ea78e2648df82832c15291bc6a7",
            "2a2d0b98a117421fa7cc1dcbc9dcf5ed"
          ]
        },
        "id": "OPSTqnBoUWCr",
        "outputId": "63aafd7d-3854-46a3-ccdb-c1c2067e7fe7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/deit/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0d880d1fc244d9585a408b88082fcbe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/330M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scripting DeiT\n",
        "\n",
        "To use the model on mobile, we first need to script the model. See the Script and Optimize recipe for a quick overview. Run the code below to convert the DeiT model used in the previous step to the TorchScript format that can run on mobile."
      ],
      "metadata": {
        "id": "iZ4GwkM9VYdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "scripted_model = torch.jit.script(model)\n",
        "scripted_model.save(\"fbdeit_scripted.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFU9ed2YU6be",
        "outputId": "be7b6a47-e77f-42ff-998d-ea73753b4f72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantizing DeiT\n",
        "\n",
        "**Quantization 정리**: [링크 참조](\n",
        "https://velog.io/@jooh95/%EB%94%A5%EB%9F%AC%EB%8B%9D-Quantization%EC%96%91%EC%9E%90%ED%99%94-%EC%A0%95%EB%A6%AC)\n",
        "</br>-> 한마디로 inference 시, 훈련된 소수점 형태인 모델 가중치들을 정수형으로 바꿈으로써, 모델의 속도 상향시키고와 사이즈를 줄이는 방법\n",
        " (다만, 성능은 줄어들 수 있음)  "
      ],
      "metadata": {
        "id": "LHQQECJ_W12y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 'fbgemm' for server inference and 'qnnpack' for mobile inference\n",
        "backend = \"fbgemm\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n",
        "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "torch.backends.quantized.engine = backend\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
        "scripted_quantized_model = torch.jit.script(quantized_model)\n",
        "scripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzv2y6TFVf9q",
        "outputId": "046fa6d4-b699-428e-8d6b-2f8bbb675b52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = scripted_quantized_model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item())\n",
        "# The same output 269 should be printed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0XrlE6bYBNE",
        "outputId": "d20013ea-7248-4756-df9d-2e1b09ff2b2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing DeiT\n",
        "\n",
        "The final step before using the quantized and scripted model on mobile is to optimize it"
      ],
      "metadata": {
        "id": "UtBRhOYdYgdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
        "optimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")"
      ],
      "metadata": {
        "id": "ezBpv_ApYK2q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = optimized_scripted_quantized_model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item())\n",
        "# Again, the same output 269 should be printed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDReTEQaY2XW",
        "outputId": "db30a3f2-ca32-46d4-f7a7-aa79e61e213e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1102: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1408.)\n",
            "  return forward_call(*input, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Lite Interpreter\n",
        "\n",
        "Although the lite model size is comparable to the non-lite version, when running the lite version on mobile, the inference speed up is expected."
      ],
      "metadata": {
        "id": "yp34CczdZOoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\n",
        "ptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")"
      ],
      "metadata": {
        "id": "FpPFLYFbY5Ij"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Inference Speed"
      ],
      "metadata": {
        "id": "DQdV4DvYZmvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n",
        "    out = model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof2:\n",
        "    out = scripted_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof3:\n",
        "    out = scripted_quantized_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof4:\n",
        "    out = optimized_scripted_quantized_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof5:\n",
        "    out = ptl(img)\n",
        "\n",
        "print(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\n",
        "print(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\n",
        "print(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\n",
        "print(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\n",
        "print(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWiaOdKTZZy6",
        "outputId": "c0d2d04b-1ae4-419b-d40a-9752ab62b436"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original model: 663.23ms\n",
            "scripted model: 694.24ms\n",
            "scripted & quantized model: 427.35ms\n",
            "scripted & quantized & optimized model: 506.47ms\n",
            "lite model: 474.93ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\n",
        "df = pd.concat([df, pd.DataFrame([\n",
        "    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n",
        "    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
        "    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
        "    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
        "    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n",
        "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n",
        "    columns=['Inference Time', 'Reduction'])], axis=1)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYpoE0cNZtNp",
        "outputId": "e7887831-632c-4989-8532-14eef05feb9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Model Inference Time Reduction\n",
            "0                          original model       663.23ms        0%\n",
            "1                          scripted model       694.24ms    -4.68%\n",
            "2              scripted & quantized model       427.35ms    35.57%\n",
            "3  scripted & quantized & optimized model       506.47ms    23.64%\n",
            "4                              lite model       474.93ms    28.39%\n"
          ]
        }
      ]
    }
  ]
}