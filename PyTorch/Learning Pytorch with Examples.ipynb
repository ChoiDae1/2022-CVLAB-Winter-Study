{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up:numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 292.30575244863275\n",
      "199 197.51938317673148\n",
      "299 134.4778416008033\n",
      "399 92.53396385975952\n",
      "499 64.61624301054233\n",
      "599 46.02662318818138\n",
      "699 33.642931220611175\n",
      "799 25.38960230866069\n",
      "899 19.886365435793778\n",
      "999 16.214994050184977\n",
      "1099 13.7644029652506\n",
      "1199 12.127746693546108\n",
      "1299 11.034041716703364\n",
      "1399 10.302715354832628\n",
      "1499 9.813383949178863\n",
      "1599 9.485750326102597\n",
      "1699 9.266227238328907\n",
      "1799 9.119032845040127\n",
      "1899 9.02026065539507\n",
      "1999 8.953928597941486\n",
      "Result y = 0.006491503952927997 + 0.8470612893989192 x + -0.0011198924509721138 x^2 + -0.09195355682064425 x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # forward pass: compute predicted y\n",
    "    # y= a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b*x + c*x**2 + d*x**3\n",
    "\n",
    "    # compute and print loss\n",
    "    loss = np.square(y_pred - y).sum() #제곱합\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2 * (y_pred - y) \n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x**2).sum()\n",
    "    grad_d = (grad_y_pred * x**3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 6605.0751953125\n",
      "199 4411.7978515625\n",
      "299 2949.44775390625\n",
      "399 1973.9442138671875\n",
      "499 1322.8597412109375\n",
      "599 888.06103515625\n",
      "699 597.529541015625\n",
      "799 403.2779235839844\n",
      "899 273.3160705566406\n",
      "999 186.308349609375\n",
      "1099 128.01702880859375\n",
      "1199 88.93572235107422\n",
      "1299 62.71382522583008\n",
      "1399 45.1061897277832\n",
      "1499 33.273170471191406\n",
      "1599 25.31415557861328\n",
      "1699 19.956092834472656\n",
      "1799 16.345714569091797\n",
      "1899 13.910737991333008\n",
      "1999 12.266877174377441\n",
      "Result: y = -0.037339773029088974 + 0.8110913038253784 x + 0.006441730540245771 x^2 + -0.08683713525533676 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(3758.8455, grad_fn=<SumBackward0>)\n",
      "199 tensor(2497.3796, grad_fn=<SumBackward0>)\n",
      "299 tensor(1660.6471, grad_fn=<SumBackward0>)\n",
      "399 tensor(1105.5250, grad_fn=<SumBackward0>)\n",
      "499 tensor(737.1539, grad_fn=<SumBackward0>)\n",
      "599 tensor(492.6509, grad_fn=<SumBackward0>)\n",
      "699 tensor(330.3240, grad_fn=<SumBackward0>)\n",
      "799 tensor(222.5258, grad_fn=<SumBackward0>)\n",
      "899 tensor(150.9190, grad_fn=<SumBackward0>)\n",
      "999 tensor(103.3390, grad_fn=<SumBackward0>)\n",
      "1099 tensor(71.7139, grad_fn=<SumBackward0>)\n",
      "1199 tensor(50.6865, grad_fn=<SumBackward0>)\n",
      "1299 tensor(36.7007, grad_fn=<SumBackward0>)\n",
      "1399 tensor(27.3948, grad_fn=<SumBackward0>)\n",
      "1499 tensor(21.2004, grad_fn=<SumBackward0>)\n",
      "1599 tensor(17.0755, grad_fn=<SumBackward0>)\n",
      "1699 tensor(14.3275, grad_fn=<SumBackward0>)\n",
      "1799 tensor(12.4959, grad_fn=<SumBackward0>)\n",
      "1899 tensor(11.2745, grad_fn=<SumBackward0>)\n",
      "1999 tensor(10.4596, grad_fn=<SumBackward0>)\n",
      "Result: y = -0.017310447990894318 + 0.820694625377655 x + 0.002986340783536434 x^2 + -0.08820312470197678 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Defining new autograd functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(797.4432, grad_fn=<SumBackward0>)\n",
      "199 tensor(648.5474, grad_fn=<SumBackward0>)\n",
      "299 tensor(528.5093, grad_fn=<SumBackward0>)\n",
      "399 tensor(420.1289, grad_fn=<SumBackward0>)\n",
      "499 tensor(332.4771, grad_fn=<SumBackward0>)\n",
      "599 tensor(283.4982, grad_fn=<SumBackward0>)\n",
      "699 tensor(253.5729, grad_fn=<SumBackward0>)\n",
      "799 tensor(231.1326, grad_fn=<SumBackward0>)\n",
      "899 tensor(212.5706, grad_fn=<SumBackward0>)\n",
      "999 tensor(196.3892, grad_fn=<SumBackward0>)\n",
      "1099 tensor(181.8755, grad_fn=<SumBackward0>)\n",
      "1199 tensor(168.6630, grad_fn=<SumBackward0>)\n",
      "1299 tensor(156.5443, grad_fn=<SumBackward0>)\n",
      "1399 tensor(145.3870, grad_fn=<SumBackward0>)\n",
      "1499 tensor(135.0951, grad_fn=<SumBackward0>)\n",
      "1599 tensor(125.5920, grad_fn=<SumBackward0>)\n",
      "1699 tensor(116.8127, grad_fn=<SumBackward0>)\n",
      "1799 tensor(108.6998, grad_fn=<SumBackward0>)\n",
      "1899 tensor(101.2012, grad_fn=<SumBackward0>)\n",
      "1999 tensor(94.2693, grad_fn=<SumBackward0>)\n",
      "Result: y = 0.00034738899557851255 + -1.5705084800720215 * P3(4.448800154932542e-06 + 0.25003358721733093 x)\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # To apply our function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "    \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 441.8584289550781\n",
      "199 302.73828125\n",
      "299 208.5313720703125\n",
      "399 144.67039489746094\n",
      "499 101.33369445800781\n",
      "599 71.8927993774414\n",
      "699 51.869876861572266\n",
      "799 38.23670196533203\n",
      "899 28.943798065185547\n",
      "999 22.60218048095703\n",
      "1099 18.26947593688965\n",
      "1199 15.305959701538086\n",
      "1299 13.276647567749023\n",
      "1399 11.8854398727417\n",
      "1499 10.930571556091309\n",
      "1599 10.274443626403809\n",
      "1699 9.8231201171875\n",
      "1799 9.512300491333008\n",
      "1899 9.298022270202637\n",
      "1999 9.150129318237305\n",
      "Result: y = -0.01600782759487629 + 0.8468155860900879 x + 0.0027616159059107304 x^2 + -0.09191860258579254 x^3\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p) # 2000 x 3\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1) # torch.nn.Flatten(start_dim=0, end_dim=1) -> 1D tensor로 바뀜\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2723.814697265625\n",
      "199 2716.7783203125\n",
      "299 2710.35888671875\n",
      "399 2704.13427734375\n",
      "499 2697.9677734375\n",
      "599 2691.850341796875\n",
      "699 2685.740966796875\n",
      "799 2679.64013671875\n",
      "899 2673.547119140625\n",
      "999 2667.46728515625\n",
      "1099 2661.4072265625\n",
      "1199 2655.35546875\n",
      "1299 2649.31201171875\n",
      "1399 2643.277099609375\n",
      "1499 2637.25\n",
      "1599 2631.23095703125\n",
      "1699 2625.220458984375\n",
      "1799 2619.21826171875\n",
      "1899 2613.22412109375\n",
      "1999 2607.23828125\n",
      "Result: y = -0.08636999875307083 + 0.29382213950157166 x + -0.07492367178201675 x^2 + 0.07046401500701904 x^3\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p) # 2000 x 3\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1) # torch.nn.Flatten(start_dim=0, end_dim=1) -> 1D tensor로 바뀜\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x **3\n",
    "    \n",
    "    def string(self):\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 8069.37060546875\n",
      "199 7598.2109375\n",
      "299 7265.4375\n",
      "399 6966.8251953125\n",
      "499 6683.89990234375\n",
      "599 6413.07080078125\n",
      "699 6153.33056640625\n",
      "799 5904.1435546875\n",
      "899 5665.064453125\n",
      "999 5435.68359375\n",
      "1099 5215.60400390625\n",
      "1199 5004.44970703125\n",
      "1299 4801.85791015625\n",
      "1399 4607.4833984375\n",
      "1499 4420.99072265625\n",
      "1599 4242.060546875\n",
      "1699 4070.38671875\n",
      "1799 3905.6748046875\n",
      "1899 3747.642822265625\n",
      "1999 3596.0185546875\n",
      "Result: y = 0.034619301557540894 + -0.987536609172821 x + -0.005972400773316622 x^2 + 0.1690019965171814 x^3\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = Polynomial3()\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-8, momentum=0.9)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Control Flow + Weight Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(())) #weight sharing\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "    \n",
    "    def string(self):\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 5301.6708984375\n",
      "199 3124.14501953125\n",
      "299 1017.3937377929688\n",
      "399 683.411865234375\n",
      "499 960.5345458984375\n",
      "599 491.486083984375\n",
      "699 452.1693420410156\n",
      "799 443.3489685058594\n",
      "899 463.5733337402344\n",
      "999 411.8963317871094\n",
      "1099 413.3352355957031\n",
      "1199 386.67694091796875\n",
      "1299 362.6190185546875\n",
      "1399 348.4512023925781\n",
      "1499 348.0978698730469\n",
      "1599 327.8086242675781\n",
      "1699 331.91448974609375\n",
      "1799 305.2432556152344\n",
      "1899 304.955322265625\n",
      "1999 294.56170654296875\n",
      "Result: y = -0.5461016893386841 + 0.7269793748855591 x + 0.09858264774084091 x^2 + -0.0732816532254219 x^3 + -0.008473627269268036 x^4 ? + -0.008473627269268036 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = DynamicNet()\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-8, momentum=0.9)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: {model.string()}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c46e020bd7951a01c610897444e7b62e6d637ce0a919f4ddb4c600ef01938ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('codingstudy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
