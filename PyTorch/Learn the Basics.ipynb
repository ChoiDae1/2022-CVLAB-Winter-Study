{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUICKSTART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "#Create data loaders.\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_loader:\n",
    "    print(\"Shape of X: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) #dataset's size\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y  = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader) #num_batches\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.312349 [    0/60000]\n",
      "loss: 2.295511 [ 6400/60000]\n",
      "loss: 2.279480 [12800/60000]\n",
      "loss: 2.265812 [19200/60000]\n",
      "loss: 2.250063 [25600/60000]\n",
      "loss: 2.216735 [32000/60000]\n",
      "loss: 2.218797 [38400/60000]\n",
      "loss: 2.183095 [44800/60000]\n",
      "loss: 2.185128 [51200/60000]\n",
      "loss: 2.150813 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 2.146841 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 2.160218 [    0/60000]\n",
      "loss: 2.146248 [ 6400/60000]\n",
      "loss: 2.095606 [12800/60000]\n",
      "loss: 2.106461 [19200/60000]\n",
      "loss: 2.055187 [25600/60000]\n",
      "loss: 1.997883 [32000/60000]\n",
      "loss: 2.011998 [38400/60000]\n",
      "loss: 1.938118 [44800/60000]\n",
      "loss: 1.947298 [51200/60000]\n",
      "loss: 1.863931 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 1.873350 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.908638 [    0/60000]\n",
      "loss: 1.874515 [ 6400/60000]\n",
      "loss: 1.771200 [12800/60000]\n",
      "loss: 1.802149 [19200/60000]\n",
      "loss: 1.692675 [25600/60000]\n",
      "loss: 1.653376 [32000/60000]\n",
      "loss: 1.650335 [38400/60000]\n",
      "loss: 1.565078 [44800/60000]\n",
      "loss: 1.595028 [51200/60000]\n",
      "loss: 1.476294 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.509268 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.579209 [    0/60000]\n",
      "loss: 1.542840 [ 6400/60000]\n",
      "loss: 1.405306 [12800/60000]\n",
      "loss: 1.469093 [19200/60000]\n",
      "loss: 1.354928 [25600/60000]\n",
      "loss: 1.356465 [32000/60000]\n",
      "loss: 1.351142 [38400/60000]\n",
      "loss: 1.283754 [44800/60000]\n",
      "loss: 1.324967 [51200/60000]\n",
      "loss: 1.219620 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.252953 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.333075 [    0/60000]\n",
      "loss: 1.312946 [ 6400/60000]\n",
      "loss: 1.154420 [12800/60000]\n",
      "loss: 1.257360 [19200/60000]\n",
      "loss: 1.136874 [25600/60000]\n",
      "loss: 1.164850 [32000/60000]\n",
      "loss: 1.171100 [38400/60000]\n",
      "loss: 1.110260 [44800/60000]\n",
      "loss: 1.156260 [51200/60000]\n",
      "loss: 1.069722 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.093839 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n---------------------')\n",
    "    train(train_loader, model, loss_fn , optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x = test_data[0][0] #1x28x28\n",
    "y = test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.9180, 0.2444],\n",
      "        [0.9509, 0.7375]])\n"
     ]
    }
   ],
   "source": [
    "#initializing a Tensor\n",
    "\n",
    "#Directly from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "#from Numpy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "#from another tensor\n",
    "x_ones = torch.ones_like(x_data)\n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4865, 0.7724, 0.2227],\n",
      "        [0.3716, 0.0967, 0.5055]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3) #tuple\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape) #torch.zeros(2,3)해도 됨.\n",
    "\n",
    "print(rand_tensor)\n",
    "print(ones_tensor)\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#Attributes of a Tensor\n",
    "\n",
    "tensor = torch.rand(3,4) #shape=(3,4)\n",
    "\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype) #datatype\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#standard numpy-like indexing and slicing\n",
    "\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor[0]) #first row\n",
    "print(tensor[:,0]) #first column\n",
    "print(tensor[:,-1]) #last column\n",
    "\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#joining tensors\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1) #(4, 4*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Arithmetric operations\n",
    "\n",
    "#This computes matrix multipliation between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "#This computes the element-wise product. z1, z2, z3, will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#single-element tensors\n",
    "\n",
    "agg = tensor.sum()\n",
    "print(agg)\n",
    "agg_item = agg.item() #convert single-element tensor into python numerical value\n",
    "print(agg_item, type(agg_item)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#In-place operations\n",
    "#In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.\n",
    "\n",
    "print(tensor, '\\n')\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Bridge with Numpy\n",
    "\n",
    "#tensor to numpy array\n",
    "t = torch.ones(5)\n",
    "print(t)\n",
    "n = t.numpy()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "#A change in the tensor reflects in the NumPy array.\n",
    "\n",
    "t.add_(1)\n",
    "print(t)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#numpy array to tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(n)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASETS & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Iterating and Visualizing the Dataset\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Custom Dataset for your files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing your data for training with DataLoaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "X, y = ds[0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD THE NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#get device for training\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_prob = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_prob.argmax(dim=1)\n",
    "print(f'Predicted class: {y_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#Model Layer\n",
    "\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size()) #input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "#nn.flatten\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "#nn.Linear\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1444, -0.0902,  0.2332,  0.1707, -0.0084,  0.1291,  0.0561,  0.1088,\n",
      "         -0.2433,  0.0797,  0.2774,  0.1349, -0.3878,  0.0555, -0.3535,  0.1815,\n",
      "          0.1567, -0.5063,  0.0633, -0.2023],\n",
      "        [ 0.0601, -0.0408,  0.4553,  0.0345, -0.1329,  0.1380,  0.1110,  0.4524,\n",
      "          0.1129,  0.0458,  0.0820, -0.3286, -0.4594, -0.3304, -0.3472, -0.4533,\n",
      "          0.1914, -0.1957, -0.2688, -0.0528],\n",
      "        [ 0.2888, -0.1292,  0.5157,  0.1389, -0.2635,  0.0891,  0.2922,  0.3085,\n",
      "          0.2931, -0.0168,  0.0272, -0.0465, -0.4188, -0.3965, -0.2496, -0.1878,\n",
      "          0.0044, -0.7833, -0.2897,  0.0432]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1444, 0.0000, 0.2332, 0.1707, 0.0000, 0.1291, 0.0561, 0.1088, 0.0000,\n",
      "         0.0797, 0.2774, 0.1349, 0.0000, 0.0555, 0.0000, 0.1815, 0.1567, 0.0000,\n",
      "         0.0633, 0.0000],\n",
      "        [0.0601, 0.0000, 0.4553, 0.0345, 0.0000, 0.1380, 0.1110, 0.4524, 0.1129,\n",
      "         0.0458, 0.0820, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.2888, 0.0000, 0.5157, 0.1389, 0.0000, 0.0891, 0.2922, 0.3085, 0.2931,\n",
      "         0.0000, 0.0272, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0044, 0.0000,\n",
      "         0.0000, 0.0432]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#nn.ReLU\n",
    "\n",
    "print(f'Before ReLU: {hidden1}\\n\\n')\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f'After ReLU: {hidden1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential \n",
    "\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Softmax\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_prob = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 3.2101e-02,  3.4539e-02,  1.3252e-02,  ...,  8.3148e-05,\n",
      "          4.8584e-03, -3.2833e-03],\n",
      "        [-9.7594e-03,  1.4417e-02, -3.2976e-02,  ..., -2.4839e-02,\n",
      "          1.6959e-02, -1.1282e-02]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0257,  0.0095], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0313, -0.0354,  0.0124,  ..., -0.0150, -0.0175, -0.0261],\n",
      "        [-0.0157, -0.0204,  0.0301,  ...,  0.0088, -0.0259, -0.0079]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0092, -0.0361], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0134,  0.0297, -0.0424,  ..., -0.0420,  0.0098, -0.0171],\n",
      "        [ 0.0040, -0.0353,  0.0052,  ...,  0.0379,  0.0439, -0.0123]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0115, -0.0305], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model Parameters\n",
    "\n",
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) #input\n",
    "y = torch.zeros(3) #target\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x00000162868138B0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x0000016286813A00>\n"
     ]
    }
   ],
   "source": [
    "# reference to the backward propagation function is stored in grad_fn property of a tensor. \n",
    "\n",
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2426, 0.3035, 0.2761],\n",
      "        [0.2426, 0.3035, 0.2761],\n",
      "        [0.2426, 0.3035, 0.2761],\n",
      "        [0.2426, 0.3035, 0.2761],\n",
      "        [0.2426, 0.3035, 0.2761]])\n",
      "tensor([0.2426, 0.3035, 0.2761])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Disabling Gradient Tracking\n",
    "# To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZING MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.307805  [    0/60000]\n",
      "loss: 2.291766  [ 6400/60000]\n",
      "loss: 2.267848  [12800/60000]\n",
      "loss: 2.254846  [19200/60000]\n",
      "loss: 2.243033  [25600/60000]\n",
      "loss: 2.206446  [32000/60000]\n",
      "loss: 2.221554  [38400/60000]\n",
      "loss: 2.182954  [44800/60000]\n",
      "loss: 2.169196  [51200/60000]\n",
      "loss: 2.143104  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 2.134328 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.145715  [    0/60000]\n",
      "loss: 2.136775  [ 6400/60000]\n",
      "loss: 2.071595  [12800/60000]\n",
      "loss: 2.085156  [19200/60000]\n",
      "loss: 2.038058  [25600/60000]\n",
      "loss: 1.970240  [32000/60000]\n",
      "loss: 2.004468  [38400/60000]\n",
      "loss: 1.916144  [44800/60000]\n",
      "loss: 1.908630  [51200/60000]\n",
      "loss: 1.850600  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.842074 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.873349  [    0/60000]\n",
      "loss: 1.848145  [ 6400/60000]\n",
      "loss: 1.720423  [12800/60000]\n",
      "loss: 1.766458  [19200/60000]\n",
      "loss: 1.673744  [25600/60000]\n",
      "loss: 1.615971  [32000/60000]\n",
      "loss: 1.647908  [38400/60000]\n",
      "loss: 1.547155  [44800/60000]\n",
      "loss: 1.568903  [51200/60000]\n",
      "loss: 1.478994  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 1.489525 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.552834  [    0/60000]\n",
      "loss: 1.527984  [ 6400/60000]\n",
      "loss: 1.370930  [12800/60000]\n",
      "loss: 1.449728  [19200/60000]\n",
      "loss: 1.352240  [25600/60000]\n",
      "loss: 1.333722  [32000/60000]\n",
      "loss: 1.356227  [38400/60000]\n",
      "loss: 1.282793  [44800/60000]\n",
      "loss: 1.313528  [51200/60000]\n",
      "loss: 1.228450  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.246087 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.316766  [    0/60000]\n",
      "loss: 1.308878  [ 6400/60000]\n",
      "loss: 1.137191  [12800/60000]\n",
      "loss: 1.246544  [19200/60000]\n",
      "loss: 1.140426  [25600/60000]\n",
      "loss: 1.148493  [32000/60000]\n",
      "loss: 1.174538  [38400/60000]\n",
      "loss: 1.115821  [44800/60000]\n",
      "loss: 1.149690  [51200/60000]\n",
      "loss: 1.075887  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.090211 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.154738  [    0/60000]\n",
      "loss: 1.166244  [ 6400/60000]\n",
      "loss: 0.978688  [12800/60000]\n",
      "loss: 1.114403  [19200/60000]\n",
      "loss: 1.002737  [25600/60000]\n",
      "loss: 1.018253  [32000/60000]\n",
      "loss: 1.057415  [38400/60000]\n",
      "loss: 1.004760  [44800/60000]\n",
      "loss: 1.038155  [51200/60000]\n",
      "loss: 0.975909  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.985787 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.037692  [    0/60000]\n",
      "loss: 1.070617  [ 6400/60000]\n",
      "loss: 0.866663  [12800/60000]\n",
      "loss: 1.023447  [19200/60000]\n",
      "loss: 0.912105  [25600/60000]\n",
      "loss: 0.924394  [32000/60000]\n",
      "loss: 0.978552  [38400/60000]\n",
      "loss: 0.930924  [44800/60000]\n",
      "loss: 0.959085  [51200/60000]\n",
      "loss: 0.907648  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.913548 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.950143  [    0/60000]\n",
      "loss: 1.003450  [ 6400/60000]\n",
      "loss: 0.785093  [12800/60000]\n",
      "loss: 0.958714  [19200/60000]\n",
      "loss: 0.850597  [25600/60000]\n",
      "loss: 0.855617  [32000/60000]\n",
      "loss: 0.922951  [38400/60000]\n",
      "loss: 0.881379  [44800/60000]\n",
      "loss: 0.901526  [51200/60000]\n",
      "loss: 0.858527  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.861506 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.882441  [    0/60000]\n",
      "loss: 0.953247  [ 6400/60000]\n",
      "loss: 0.723861  [12800/60000]\n",
      "loss: 0.910574  [19200/60000]\n",
      "loss: 0.806943  [25600/60000]\n",
      "loss: 0.804235  [32000/60000]\n",
      "loss: 0.881663  [38400/60000]\n",
      "loss: 0.846962  [44800/60000]\n",
      "loss: 0.858374  [51200/60000]\n",
      "loss: 0.821101  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.822297 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.828612  [    0/60000]\n",
      "loss: 0.913093  [ 6400/60000]\n",
      "loss: 0.676404  [12800/60000]\n",
      "loss: 0.873455  [19200/60000]\n",
      "loss: 0.773983  [25600/60000]\n",
      "loss: 0.765191  [32000/60000]\n",
      "loss: 0.849121  [38400/60000]\n",
      "loss: 0.821885  [44800/60000]\n",
      "loss: 0.824895  [51200/60000]\n",
      "loss: 0.791450  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.791270 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE AND LOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state.dict(), 'model_weights.pth') #saving model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')#structure와 weights 모두 저장\n",
    "model = torch.load('model.pth')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c46e020bd7951a01c610897444e7b62e6d637ce0a919f4ddb4c600ef01938ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('codingstudy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
